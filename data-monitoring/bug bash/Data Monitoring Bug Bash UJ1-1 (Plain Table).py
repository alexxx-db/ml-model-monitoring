# Databricks notebook source
# MAGIC %md 
# MAGIC 
# MAGIC # Data Monitoring Bug Bash UJ1-1 (Plain Table)
# MAGIC 
# MAGIC Please make a copy of this notebook before use.

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC 
# MAGIC Useful links:
# MAGIC  - [Bug Bash Doc](http://go/dm/bugbash)
# MAGIC  - [API Reference](https://docs.google.com/document/d/1LIsz1k4zD888GuLfxxsuJaB-QI1_gY4sHwi9R-lj65c)
# MAGIC  - [User Guide](https://docs.google.com/document/d/1d7BUxbxKDnQfhKZ8e0w99by9JLF4gRmAvDjVxmpE0ZU/edit#heading=h.pc8x54hv4i93)

# COMMAND ----------

# Install data monitoring client library. Eventually this will be included in MLR
%pip install "https://ml-team-public-read.s3.us-west-2.amazonaws.com/wheels/data-monitoring/a4050ef7-b183-47a1-a145-e614628e3146/databricks_data_monitoring-0.1.0-py3-none-any.whl"

# COMMAND ----------

# Let's use your user name in the table name to avoid the conflict with other people.

user=""

assert user

# Table names cannot use any special characters
sanitized_user = user.split("@")[0].replace(".","_")
table_name = f"dm_bugbash.default.{sanitized_user}_plain_table" 

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ##0. Prepare monitored table
# MAGIC 
# MAGIC You will need to be a **UC table OWNER** in order to create monitor. If you don't have a table that you can test for monitoring, here are some palin table dataset examples: 
# MAGIC 
# MAGIC <br/>
# MAGIC 
# MAGIC - **Census Income csv dataset**:
# MAGIC 
# MAGIC ```
# MAGIC import pandas as pd
# MAGIC # Load and clean train and test datasets
# MAGIC cols = ['age','workclass','fnlwgt','education','education-num','marital-status',
# MAGIC         'occupation','relationship','race','sex','capital-gain', 'capital-loss',
# MAGIC         'hours-per-week', 'native-country','income']
# MAGIC pdf = pd.read_csv(
# MAGIC   "/dbfs/databricks-datasets/adult/adult.data",
# MAGIC   names=cols,
# MAGIC   skipinitialspace=True,
# MAGIC )
# MAGIC df = spark.createDataFrame(pdf)
# MAGIC df.write.mode("overwrite").format("delta").option("delta.enableChangeDataFeed", "true").saveAsTable(table_name)
# MAGIC ```
# MAGIC <br/>
# MAGIC 
# MAGIC - **TPC-H dataset**: 
# MAGIC The tpch database contains data from the [TPC-H Benchmark](https://www.tpc.org/tpch/)
# MAGIC ```
# MAGIC spark.sql(f"CREATE TABLE {table_name} DEEP CLONE samples.tpch.customer TBLPROPERTIES (delta.enableChangeDataFeed = true);")
# MAGIC ```

# COMMAND ----------

# MAGIC 
# MAGIC %md
# MAGIC ####(Optional) 0.1 Prepare baseline table
# MAGIC - Consider to create a baseline table that the monitored table against for. One easier way is chosing a subset of monitored table.
# MAGIC ```
# MAGIC baseline_table_name=table_name + "_baseline"
# MAGIC df = spark.sql(f"SELECT * FROM {table_name} ORDER BY RAND()LIMIT 500")
# MAGIC df.write.mode("overwrite").format("delta").option("delta.enableChangeDataFeed", "true").saveAsTable(baseline_table_name)
# MAGIC ```

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ## 1. Create the monitor
# MAGIC - You can check **API Walkthrogh - Monitors** section from the [user guide](https://docs.google.com/document/d/1d7BUxbxKDnQfhKZ8e0w99by9JLF4gRmAvDjVxmpE0ZU/edit#heading=h.70vskvghxjux) for more explainations.

# COMMAND ----------

from pyspark.sql import types as T
import databricks.data_monitoring as dm
from databricks.data_monitoring.analysis import analysis_type

plain_table_analysis = analysis_type.PlainTable(
  # UJ: Fill the parameters to create plain table analysis type (check API reference).
)
dm_info = dm.create_monitor(
  # UJ: Fill the parameters to create the monitor with plain table analysis type.
)

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ## 2. Inspect the monitor
# MAGIC - You can check **Inspecting the Monitor** section from the [user guide](https://docs.google.com/document/d/1d7BUxbxKDnQfhKZ8e0w99by9JLF4gRmAvDjVxmpE0ZU/edit#heading=h.70vskvghxjux) for more explainations.

# COMMAND ----------

dm.get_monitor_info(
  # UJ: Fill the parameters to inspect the monitor by API.
)

# UJ: inspecting each artifact (e.g. output tables, dashboard) generated by the library.

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ## 3. Generate metrics
# MAGIC - For PlainTable monitors, metrics are generated each time you call the `refresh_metrics` API. In order to see some interesting, data let's call `refresh_metrics` a few times.

# COMMAND ----------

import time


for i in range(10):
  dm.refresh_metrics(
    # UJ: Fill the parameters to inspect the monitor by API.
  )
  time.sleep(5)

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ## 4. Update the monitor

# COMMAND ----------

dm.update_monitor(
  # UJ: Fill the right parameters to update the monitor.
)

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ## 5. Delete the monitor

# COMMAND ----------

dm.delete_monitor(
  # UJ: Fill the right parameters to delete the monitor.
)
